#+TITLE: Reading List
* Unread Things

[[https://applied-llms.org/][What we've learned from a year of building with LLMs]]

[[https://yang-song.net/blog/2021/score/][Generative Modeling by Estimating Gradients of the Data Distribution]]

[[https://markovbio.github.io/biomedical-progress/][A Future History of Biomedical Progress]]

[[https://roberttlange.com/posts/2020/06/lottery-ticket-hypothesis/][the lottery ticket hypothesis: a survey]]

[[https://x.com/stevenstrogatz/status/1794315234917937358][stupidity in scientific research]]

[[https://github.com/naklecha/llama3-from-scratch][llama 3 from scratch]] probably worth going through after chatgpt course

books

[[https://d2l.ai/][Dive into Deep Learning]] code examples for things

[[https://www.bishopbook.com/][Deep Learning Foundations and Concepts]] bishop book

Foundations of Computer Vision

Foundations of Applied Mathematics Volumes 1 + 2 (appropriate??)


lectures

[[https://www.youtube.com/watch?v=l8pRSuU81PU][let's reproduce gpt-2 (124M) Karpathy]] interesting to compare with the course I'm taking, no need to do now

Papers

[[https://arxiv.org/abs/2310.01889][ring attention with blockwise transformers for near infinite context]]

[[https://arxiv.org/abs/2402.17764][the era of 1-bit llms]]

[[https://arxiv.org/abs/2403.09629][quiet-STaR: language models can teach themselves to think before speaking]]
related: [[https://arxiv.org/pdf/2405.14838][From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step]]

[[https://arxiv.org/abs/2404.07143][leave no context behind: efficient infinite context transformers with infini-attention]]

[[https://arxiv.org/abs/2404.08819][the illusion of state in state-space models]]

[[https://arxiv.org/abs/2404.15146][rethinking LLM memorization through the lens of adversarial compression]]

[[https://arxiv.org/abs/2404.09562][Ïƒ-GPTs]]

[[https://arxiv.org/abs/2404.19737][better and faster llms via multi-token prediction]]

[[https://openreview.net/pdf?id=H1eerhIpLV][minigo: a case study in reproducing reinforcement learning research]]
