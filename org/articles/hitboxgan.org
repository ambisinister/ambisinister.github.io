#+TITLE: This Hitbox Does Not Exist
* This Hitbox Does Not Exist

** Introduction

Recently at work I've been getting interested in some properties of Generative Adversarial Networks (GANs), The actual properties I was looking at aren't terribly interesting to a general audience, but I figured it would be fun and worthwhile to train a few networks to generate some fun images. 

In short, I played around with GANs to generate images that looked like Super Smash Brothers Melee frame data, and the results can be found in this document.

** Dataset

To assemble this dataset, I scraped every character's moves on ssbwiki and assembled a corpus of gifs, which can be found in a zip file at [[http://tinyurl.com/every-hitbox]]. From there, I did some basic image processing to save each frame as a 256x256 square png, which was surprisingly a huge pain given how insistent openCV is on saving frames of gifs as 1-channel images. Here's some very not-optimized code I used to accomplish this task

#+BEGIN_SRC python
import os
from PIL import Image
from PIL import GifImagePlugin
import numpy as np
import cv2

from PIL import Image, ImageSequence

dataloc = "/home/ambi/Downloads/data/hitboxes/"
dataput = "/home/ambi/Downloads/data/hitboxes/dataset/pngs/"

frames = []

def pngify():
    for folder in os.listdir(dataloc):
        if folder == "dataset":
            continue
        else:
            os.mkdir(dataput+folder)
        for move in os.listdir(dataloc+folder):
            imageObject = Image.open(dataloc+folder+'/'+move)
            try:
                print("{}: {} frames".format(move, imageObject.n_frames))
                
                for i, frame in enumerate(ImageSequence.Iterator(imageObject)):
                    frame.save(dataput+folder+'/'+move.split('.')[0]+'_'+str(i)+'.png')
            except:
                print("{} not animated: adding only frame".format(move))
                frame.save(dataput+folder+'/'+move+'.png')

#+END_SRC

This yields us a corpus of 33,292 frames, which should be a solid enough baseline for training a model. 

** An Aside: Character Classification

Just to verify that things are working properly, I wrote a very simple CNN to identify which character was on screen. This wasn't much code, and could easily tell the characters apart. 

<<code>>

The harder frames are an interesting thing to look at -- with a simple out-of-the-box classifier getting very high accuracy on this relatively easy task, it's interesting to look at where the model gets confused and get some ad-hoc measure of which characters are close together and which characters are far apart. 

<<code and stuff>>

** StyleGAN

StyleGAN is an intensely media-famous GAN architecture which was notable for how well it was able to synthesize human faces (see, of course, [[https://thispersondoesnotexist.com/][This Person Does Not Exist]]). Gwern notably applied this to a large dataset of anime faces scraped off Danbooru, [[https://www.thiswaifudoesnotexist.net/][for a waifu-flavored variant on this]]. Both the original styleGAN paper ([[https://arxiv.org/pdf/1812.04948.pdf][here]]) and Gwern's writeup ([[https://www.gwern.net/TWDNE][here]]) are must-reads, but it's relatively clear styleGAN works relatively well for both real and drawn images and seems like a good starting point for our 

StyleGAN has a [[https://github.com/manicman1999/StyleGAN-Keras][Keras implementation]] floating around, which is not-entirely-faithful in some parts but probably works well enough for our purposes. I took this and with some small tweaks was able to get it training without issue. 

<<results>>

** Other Things I Tried Which Worked Less Well

*** Auxiliary Classifier GANs

I played a bit with AC-GANs for this project, because it's a relatively simple idea which yields samples which have labels associated with them, which I thought would be fun to look at ("ah yes, this blob looks like kirby" is something I caught myself doing a good amount with the styleGAN images, but it's a bit of a rorshach test -- if the label associated with an image is supposed to be Marth, we should expect it to either look close to Marth or to look horrible. 

AC-GAN generates extremely beautiful MNIST digits, but I think it doesn't scale as well for harder data since it assumes the discriminator can get decent accuracy on the images but also not overpower the generator altogether.

<<images>>

<<code, stuff>>

** References

