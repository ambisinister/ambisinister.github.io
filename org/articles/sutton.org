* Sutton and Barto

I am anticipating needing to deeply understand Reinforcement Learning to properly do my job. Everybody says that [[http://incompleteideas.net/book/RLbook2020.pdf][this book]] is the way to do that, so I'm working through it and doing the exercises. I'll be putting the exercises in this document along with some random thoughts I may have. I have done this on here [[https://planetbanatt.net/articles/probmods.html][before, with the textbook Probabilistic Models of Cognition]], so it will largely be the same as that. I am also roughly following [[https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb][the DeepMind x UCL Reinforcement Learning Course (2018)]] which follows this book. I learn much better from reading and practicing compared to from watching and listening, but it doesn't hurt to attack the problem from all fronts.

I'll be checking my answers to the exercises with LLMs as I work through the book. The majority of these are near the beginning of the book (chapter 3 has almost 30 exercises, most chapters after have about 10, part III has one exercise total). Since my objective is to understand RL well enough to do research, I'm going to make a good faith effort to actually get through the whole book, including these less exercise-heavy ones / ones not covered in the DeepMind course. We will see how well I stick to it, but I have some optimism. 

Overall I have been finding this book to be very challenging. I really thought my programming background would help me a lot more than it has been, so struggling through the necessary notation has been pretty taxing. 

** Random Thoughts

Arcade culture as multi-arm bandit problem

Evolutionary RL with parameter model merging crossover operations

spybots clone might be good environment to implement algos like MCTS

** Exercises

*** 1

Exercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the
reinforcement learning algorithm described above played against itself, with both sides
learning. What do you think would happen in this case? Would it learn a different policy
for selecting moves?

I already know self-play already works for achieving high skill in RL tasks (see: alphago) but I do think it's possible for larger environments that policies learned through self-play to tunnel into behaviors which do not fully explore a very complex environment. I remember seeing some papers exploiting this by [[https://arxiv.org/pdf/2211.00241][learning adversarial policies]] that win by creating out-of-distribution states. But for something simple like tic-tac-toe I doubt it would cause huge problems, in the limit both will explore the space completely. 

Exercise 1.2: Symmetries Many tic-tac-toe positions appear different but are really
the same because of symmetries. How might we amend the learning process described
above to take advantage of this? In what ways would this change improve the learning
process? Now think again. Suppose the opponent did not take advantage of symmetries.
In that case, should we? Is it true, then, that symmetrically equivalent positions should
necessarily have the same value?

We could take advantage of this via data augmentation in training, applying all the different rotations and mirrors to the game board and having 8 training examples instead of 1 per episode. This would make us equally strong from all angles, since we will have an equal balance of the different versions of the same positions. Taking advantage of the symmetries shouldn't have any different effects whether or not our opponent uses them or not -- they're literally identical states, and should have identical value functions as a result. One thing which may break this is if we learn that our opponent is worse at one particular rotation of the game board, and seeking to exploit that. If we do augmentation this way, we won't be able to learn that sort of thing, since we will be artificially improving their performance in those spots, but in exchange we get more training examples and better balance subject to symmetry.

Exercise 1.3: Greedy Play Suppose the reinforcement learning player was greedy, that is,
it always played the move that brought it to the position that it rated the best. Might it
learn to play better, or worse, than a nongreedy player? What problems might occur?

If the player was greedy and never explored, that could lead to lots of problems where states get assigned incorrect value estimations. For example, if you have an action which causes +1 reward 90% of the time and -1 reward 10% of the time, and you happen to get -1 reward the first time you take that option, your estimation for that action is likely to be irrecoverably wrong. The purpose of non-greedy decisions is to make sure that your "rating the best" is actually accurate, since over time you'll get a better picture of more types of states that way. 

Exercise 1.4: Learning from Exploration Suppose learning updates occurred after all
moves, including exploratory moves. If the step-size parameter is appropriately reduced
over time (but not the tendency to explore), then the state values would converge to
a different set of probabilities. What (conceptually) are the two sets of probabilities
computed when we do, and when we do not, learn from exploratory moves? Assuming
that we do continue to make exploratory moves, which set of probabilities might be better
to learn? Which would result in more wins?

Consider a case where you have a state which is next to the goal state, and also next to a state which kills you. In the greedy case, you would love to be in this state, because then you can go to the goal state, and you'll get very high reward. But in the exploration-enabled case, this state is kind of risky because there's a change you'll decide to explore in this state, land in the death state, and recieve a big negative reward. If you learn after exploratory moves, you're learning the value of the state including that probability of randomly dying, whereas otherwise you aren't. If you want to keep doing this exploration, you might be better off using these probabilities, since avoiding death is pretty important, but if you plan on turning those moves off then there's no need to avoid this hypothetical state.

Exercise 1.5: Other Improvements Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem
as posed?

Arming the player with search seems like a good way to get better estimates of the current state.

/status: checked with o1-preview/

*** 2

Exercise 2.1 In \epsilon -greedy action selection, for the case of two actions and \epsilon = 0.5, what is
the probability that the greedy action is selected?

.75 (from greedy 0.5, from random 0.5 / 2) 

Exercise 2.2: Bandit example Consider a k-armed bandit problem with k = 4 actions,
denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using
"-greedy action selection, sample-average action-value estimates, and initial estimates
of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1,
R1 = 1, A2 = 2, R2 = 1, A3 = 2, R3 = 2, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some
of these time steps the " case may have occurred, causing an action to be selected at
random. On which time steps did this definitely occur? On which time steps could this
possibly have occurred? ⇤

| Step | Action | Reward | Greedy ?                                                             |
|------+--------+--------+----------------------------------------------------------------------|
|    1 |      1 |      1 | Maybe (all 0)                                                        |
|    2 |      2 |      1 | No (A1 has 1)                                                        |
|    3 |      2 |      2 | Maybe (A1 and A2 both with 1)                                        |
|    4 |      2 |      2 | Maybe (A2 clearly best now but can still be selected by exploration) |
|    5 |      3 |      0 | No (No info on 3 at all)                                             |

Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in
the long run in terms of cumulative reward and probability of selecting the best action?
How much better will it be? Express your answer quantitatively.

In the long run. \eps = 0.01 will perform best. \eps = 0.1 learns the
optimal action the fastest, but is bottlenecked by the fact that it
must select a random action 10% of the time, meaning it gets optimal
reward 91% of the time. In comparison, once \eps = 0.01 learns the
optimal action, it will pick that option 99.5% of the time. You can
observe this in the slopes of the figure, where 0.01 is initially
lower but continues to grow.

Exercise 2.4 If the step-size parameters, \alpha_n, are not constant, then the estimate Qn is
a weighted average of previously received rewards with a weighting different from that
given by (2.6). What is the weighting on each prior reward for the general case, analogous
to (2.6), in terms of the sequence of step-size parameters? ⇤

If you expand out the terms you'll get:

Q_{n+1} = a_1 R_n + (1 - a_1)(a_2 R_{n-1} + (1 - a_2)(a_3 R_{n-2} + (1 - a_3)(Q_{n-3})))

etc

if we try to break it apart we get

Q_{n+1} = a_1 R_n + (1 - a_1)(a_2 R_{n-1}) + (1 - a_1)(1 - a_2)(a_3 R_{n-2}) + (1-a_1)(1-a_2)(1 - a_3)(Q_{n-3})))

suggesting that in the general case we arrive at a form that looks like this:

W_{n+1} = \Prod{i=1}{n} (1 - a_i) (a_n R_{n})

I think I likely have the notation wrong here but visually it makes sense.

/o1-preview: $Q_{n+1} = \sum_{k=1}^{n}(\alpha_k \prod_{i=k+1}^{n}(1-\alpha_i))R_k$/ 

Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the
diculties that sample-average methods have for nonstationary problems. Use a modified
version of the 10-armed testbed in which all the q_*(a) start out equal and then take
independent random walks (say by adding a normally distributed increment with mean 0
and standard deviation 0.01 to all the q⇤(a) on each step). Prepare plots like Figure 2.2
for an action-value method using sample averages, incrementally computed, and another
action-value method using a constant step-size parameter, \alpha = 0.1. Use \epsilon = 0.1 and
longer runs, say of 10,000 steps

#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt

# define k armed bandit
k = 10
q_stars = [5 for _ in range(k)]

def run_experiment(epsilon, method='constant'):

    num_acts = [0 for _ in q_stars]
    q_vals = [0 for _ in q_stars]

    if method == 'constant':
        alpha = 0.1

    steps = 10000

    avg_rewards = []
    pct_optimals = []
    avg_reward = 0
    optimal_actions = 0

    for step in range(steps):
        if method != 'constant':
            alpha = 1 / (step + 1)

        # random walks
        for i, bandit in enumerate(q_stars):
            q_stars[i] += np.random.normal(0, 0.01)

        # epsilon-greedy
        if np.random.random() < epsilon:
            act = np.random.randint(0, k)
        else:
            act = np.argmax(q_vals)

        num_acts[act] += 1
        q_vals[act] += alpha * (q_stars[act] - q_vals[act])

        avg_reward += (1 / (step + 1)) * q_stars[act]
        avg_rewards.append(avg_reward)

        if act == np.argmax(q_stars):
            optimal_actions += 1

        pct_optimal = optimal_actions / (step + 1)
        pct_optimals.append(pct_optimal)

    return avg_rewards, pct_optimals

const_rewards, const_optimals = run_experiment(0.1, method='constant')
avg_rewards, avg_optimals = run_experiment(0.1, method='average')

plt.title("Average rewards")
plt.plot(const_rewards, label='constant alpha')
plt.plot(avg_rewards, label='averaging')
plt.legend()
plt.show()

plt.title("Optimal actions")
plt.plot(const_optimals, label='constant alpha')
plt.plot(avg_optimals, label='averaging')
plt.legend()
plt.show()
#+END_SRC

Exercise 2.6: Mysterious Spikes The results shown in Figure 2.3 should be quite reliable
because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks.
Why, then, are there oscillations and spikes in the early part of the curve for the optimistic
method? In other words, what might make this method perform particularly better or
worse, on average, on particular early steps? ⇤

If the rewards are optimistic, it's very likely that you will pull all the levers once after only a few turns, since you'll be disappointed each time. You should then get a good picture of the best one very quickly, which means you should pick the best option very often very early on. However, you run into a problem -- greedily picking that option will make your estimate of that state worse, so by picking it you temporarily make it less likely to be selected again. This will continue until the estimates are accurate enough for selecting the best option to not make the estimate worse than the estimates for the other options.

Exercise 2.7: Unbiased Constant-Step-Size Trick In most of this chapter we have used
sample averages to estimate action values because sample averages do not produce the
initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample
averages are not a completely satisfactory solution because they may perform poorly
on nonstationary problems. Is it possible to avoid the bias of constant step sizes while
retaining their advantages on nonstationary problems? One way is to use a step size of

$\beta_n \doteq \alpha / \bar{o}_n$

to process the nth reward for a particular action, where \alpha > 0 is a conventional constant
step size, and ¯on is a trace of one that starts at 0:

$\bar{o}_n \doteq \bar{o}_{n-1} + \alpha (1 - \bar{p}_{n-1}) \text{ for } n > 0, \text{ with } \bar{o}_0 \doteq 0$.

Carry out an analysis like that in (2.6) to show that Qn is an exponential recency-weighted
average without initial bias.

#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt

# define k armed bandit
k = 10
q_stars = [np.random.normal(0, 1) for _ in range(k)]

def run_experiment(epsilon, method='constant'):

    num_acts = [0 for _ in q_stars]
    q_vals = [5 for _ in q_stars] #optimistic reward

    if method == 'constant':
        alpha = 0.1
        o_bar = 0

    steps = 10000

    avg_rewards = []
    pct_optimals = []
    avg_reward = 0
    optimal_actions = 0

    for step in range(steps):
        if method == 'constant':
            o_bar += alpha * (1 - o_bar)
            beta = alpha / o_bar
        else:
            beta = 1 / (step + 1)

        # epsilon-greedy
        if np.random.random() < epsilon:
            act = np.random.randint(0, k)
        else:
            act = np.argmax(q_vals)

        num_acts[act] += 1
        q_vals[act] += beta * (q_stars[act] - q_vals[act])

        avg_reward += (1 / (step + 1)) * q_stars[act]
        avg_rewards.append(avg_reward)

        if act == np.argmax(q_stars):
            optimal_actions += 1

        pct_optimal = optimal_actions / (step + 1)
        pct_optimals.append(pct_optimal)

    return avg_rewards, pct_optimals

const_rewards, const_optimals = run_experiment(0.1, method='constant')
avg_rewards, avg_optimals = run_experiment(0.1, method='average')

plt.title("Average rewards")
plt.plot(const_rewards, label='constant alpha')
plt.plot(avg_rewards, label='averaging')
plt.legend()
plt.show()

plt.title("Optimal actions")
plt.plot(const_optimals, label='constant alpha')
plt.plot(avg_optimals, label='averaging')
plt.legend()
plt.show()
#+END_SRC

/TODO: I think this question requires me to show that the weights sum to 1, not to implement it/

Exercise 2.8: UCB Spikes In Figure 2.4 the UCB algorithm shows a distinct spike
in performance on the 11th step. Why is this? Note that for your answer to be fully
satisfactory it must explain both why the reward increases on the 11th step and why it
decreases on the subsequent steps. Hint: If c = 1, then the spike is less prominent. ⇤

If you have 10 bandits after only a few trials, the UCB term will likely dominate for untested bandits, so it will test all the bandits once each in the first ten trials. On the 11th trial, all of the UCB terms will be equal, so it's very likely to pull the bandit which returned the highest value, which is most often the optimal one. However, once you do that, you reduce the UCB term for that bandit, which means that you'll start wanting to pull the other bandits again. This will repeat until the UCB term goes to ~0 after many trials. When c=1, this term is less dominating, so it becomes more possible to select two bandits twice in the first 10 trials, which would diffuse this spike to adjacent timesteps.

Exercise 2.9 Show that in the case of two actions, the soft-max distribution is the same
as that given by the logistic, or sigmoid, function often used in statistics and artificial
neural networks.

with two actions we have

e^{z_i} / \sum{j=1}{K} e^{z_j}

e^{z_i} / (e^{z_1} + e^{z_2})

p(1) + p(2) = 1

p(1) = e^{z_1} / (e^{z_1} + e^{z_2})

dividing numerator and denomenator by e^z_2 is equivalent to subtraction

p(1) = e^{z_1 - z_2} / (e^{z_1 - z_2} + e^{z_2 - z_2})

p(1) = e^{z_1 - z_2} / (1 + e^{z_1 - z_2})

if x = z_1 - z_2 we now have

e^x / (1 + e^x)

which is the sigmoid

Exercise 2.10 Suppose you face a 2-armed bandit task whose true action values change
randomly from time step to time step. Specifically, suppose that, for any time step,
the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case
A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case
you face at any step, what is the best expected reward you can achieve and how should
you behave to achieve it? Now suppose that on each step you are told whether you are
facing case A or case B (although you still don’t know the true action values). This is an
associative search task. What is the best expected reward you can achieve in this task,
and how should you behave to achieve it?

If you don't know the state, you do the same on both cases. picking action A will give you (10 + 90)/2 = 50 and action B will give you (20 + 80)/2 = 50 on average, so you can't do better than random. If you know what state you're in, you will want to select 2 in case A and 1 in case B, which will give you (20 + 90) / 2 = 55 average reward. Once you know the state, you collapse to the normal learning problem in a k-armed bandit, so any of those methods would work once you know the underlying state. 

Exercise 2.11 (programming) Make a figure analogous to Figure 2.6 for the nonstationary
case outlined in Exercise 2.5. Include the constant-step-size \epsilon-greedy algorithm with
\alpha = 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and
parameter setting, use the average reward over the last 100,000 steps.

#+BEGIN_SRC python
import numpy as np
import matplotlib.pyplot as plt

# define k armed bandit
k = 10
q_stars = [np.random.normal(0, 1) for _ in range(k)]

def run_experiment(epsilon, method='constant'):

    num_acts = [0 for _ in q_stars]

    if method == 'optimistic':
        q_vals = [5 for _ in q_stars]
    else:
        q_vals = [0 for _ in q_stars]

    #do they want the unbiased one?
    if method == 'constant' or method == 'optimistic': 
        alpha = 0.1
        o_bar = 0

    steps = 200000

    avg_rewards = []
    pct_optimals = []
    avg_reward = 0
    optimal_actions = 0

    for step in range(steps):
        if method == 'constant' or method == 'optimistic':
            o_bar += alpha * (1 - o_bar)
            beta = alpha / o_bar
        else:
            beta = 1 / (step + 1)

        # epsilon-greedy
        if method != 'ucb' and np.random.random() < epsilon:
            act = np.random.randint(0, k)
        elif method == 'ucb':
            ucbs = [q_vals[i] + np.sqrt(epsilon * np.log(step+1) / \
                                        num_acts[i]) for i in range(k)]
            act = np.argmax(ucbs)
        else:
            act = np.argmax(q_vals)

        num_acts[act] += 1
        q_vals[act] += beta * (q_stars[act] - q_vals[act])

        avg_reward += (1 / (step + 1)) * q_stars[act]
        avg_rewards.append(avg_reward)

        if act == np.argmax(q_stars):
            optimal_actions += 1

        pct_optimal = optimal_actions / (step + 1)
        pct_optimals.append(pct_optimal)

    return np.mean(avg_rewards[:100000])


vals = [1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4]

const_rewards = [run_experiment(x, method='constant') for x in vals]
optimistic_rewards = [run_experiment(x, method='optimistic') for x in vals]
ucb_rewards = [run_experiment(x, method='ucb') for x in vals]

plt.title("Parameter Study")
plt.plot(vals, const_rewards, label='eps-greedy')
plt.plot(vals, optimistic_rewards, label='optimistic eps-greedy')
plt.plot(vals, ucb_rewards, label='UCB')
plt.xlabel("epsilon")
plt.ylabel("Average reward over last 100k steps")
plt.legend()
plt.show()
#+END_SRC

/status: checked with o1-preview, need to fix 2.7 because code is not the correct deliverable/

*** 3

Exercise 3.1 Devise three example tasks of your own that fit into the MDP framework,
identifying for each its states, actions, and rewards. Make the three examples as different
from each other as possible. The framework is abstract and flexible and can be applied in
many different ways. Stretch its limits in some way in at least one of your examples. ⇤

1. Chess can be framed as an MDP, where each state is a board position, each action is the legal moves you can perform in that position, and each reward is the relative value of the position (or just 1 for goal state and -1 for loss state)

2. Flirting with someone can be framed as an MDP, where each state is the current point in a conversation, each action is what you can say at that point, and the reward is how much you observe they're into what you're saying (can be negative, for example if you start talking about how flirting is a Markov Decision Process)

3. Doing the exercises in Sutton and Barto can be framed as an MDP. Each state is your current location in the textbook, each action is your letter by letter solving of the problem (e.g. you write answers one letter at a time), and each reward is the feedback from a teacher or LLM about how well you solved an exercise.   

Exercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed
learning tasks? Can you think of any clear exceptions? ⇤

Maybe not usefully; a big component of this is that MDPs have the markov property (where the past sequence of events is priced into the current state, and two identical "states" which would have different local behaviors based on the path required to reach them would get represented as different states). It's possible there are MDPs it's hard to represent the state as being independent of / inclusive of the entire history prior (i.e. it is possible, but the state space is so large that the dynamics can't be learned well). [[https://en.wikipedia.org/wiki/AlphaStar_(software)][Starcraft]] might be one of these? They struggled to reach superhuman play under human constraints and had to rely on imitation learning due to the overly large state space, due to the "exploration problem". 

Exercise 3.3 Consider the problem of driving. You could define the actions in terms of
the accelerator, steering wheel, and brake, that is, where your body meets the machine.
Or you could define them farther out—say, where the rubber meets the road, considering
your actions to be tire torques. Or you could define them farther in—say, where your
brain meets your body, the actions being muscle twitches to control your limbs. Or you
could go to a really high level and say that your actions are your choices of where to drive.
What is the right level, the right place to draw the line between agent and environment?
On what basis is one location of the line to be preferred over another? Is there any
fundamental reason for preferring one location over another, or is it a free choice? ⇤

I imagine your framing matters a lot here. If you want to build a system which outperforms humans at driving, you'll likely be interested in defining it at the machine level (unless you were building a humanoid robot which drives) because in that case you're able to directly actuate the pedals and stuff. If you're building a gps navigation service which arrives at a location while avoiding the most traffic, you don't actually care about the machine at all. If you're drunk at a bar, you hopefully would carefully consider that your body's condition introduces an additional level of uncertainty to your observations and actions, even though your car in the parking lot didn't change at all. It's not so much that it's a free choice, rather that it depends on the type of problem you are attempting to solve with your agent.

Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s', r|s, a). It
should have columns for s, a, s', r, and p(s', r|s, a), and a row for every 4-tuple for which
p(s', r|s, a) > 0.

| s    | a        | s'   | r        | p(s' / s, a) | p(s', r / s, a)                    |
|------+----------+------+----------+--------------+------------------------------------|
| high | search   | high | r_search | \alpha       | \alpha * p(r = R / s, a, s')       |
| high | search   | low  | r_search | 1 - \alpha   | (1 - \alpha) * p(r = R / s, a, s') |
| low  | search   | high | -3       | 1 - \beta    | (1 - \beta) * p(r = R / s, a, s')  |
| low  | search   | low  | r_search | \beta        | \beta * p(r = R / s, a, s')        |
| high | wait     | high | r_wait   | 1            | 1 * p(r = R / s, a, s')            |
| low  | wait     | low  | r_wait   | 1            | 1 * p(r = R / s, a, s')            |
| low  | recharge | high | 0        | 1            | 1 * p(r = R / s, a, s')            |

I am a bit confused by this because it doesn't look like there's anything about the probability of a specific reward, but I guess in concept it should be this right? 

Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be
modified (very slightly) to apply to episodic tasks. Show that you know the modifications
needed by giving the modified version of (3.3).

continuing case:

$\sum_{s' \in S} \sum_{r \in R} p(s', r | s, a) = 1 \text{ for all } s \in S, a \in A(s)$

episodic case:

$\sum_{s' \in S \cup T} \sum_{r \in R} p(s', r | s, a) = 1 \text{ for all } s \in S, a \in A(s) \text{ where T is the set of terminal states }$

Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used
discounting, with all rewards zero except for -1 upon failure. What then would the
return be at each time? How does this return differ from that in the discounted, continuing
formulation of this task? ⇤

$G_t = \sum_{k=0}^{T} \gamma^k R_{t+k+1}$

Since R is always 0 except at the terminal state, we can just write this simply as

$G_t = -\gamma^T$

This differs from the discounted, continuing formulation of this task because the reward in the continuous case the model will get negative reward every time it's not balancing, but if it falls it can right itself again to resume having no penalty. In the episodic case, it will just reset so that you start again, and you're directly maximizing the time to first failure rather than the minimum number of failures as late as possible.

Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a
reward of +1 for escaping from the maze and a reward of zero at all other times. The task
seems to break down naturally into episodes—the successive runs through the maze—so
you decide to treat it as an episodic task, where the goal is to maximize expected total
reward (3.7). After running the learning agent for a while, you find that it is showing
no improvement in escaping from the maze. What is going wrong? Have you effectively
communicated to the agent what you want it to achieve? ⇤

If you do this, the agent will try to get out of the maze eventually, with no rush at all for how long that takes. As a result, with a long enough time horizon, taking enough random actions will eventually reach the terminal state, and all trials will have the same reward (+1). You aren't making it learn the maze, you're just asking it to exist until the terminal state is reached, and then rewarding it. What you would prefer is punishing -1 for every time step, so that the agent is rewarded for getting out faster, which will incentivize it to actually learn to escape the maze. 

Exercise 3.8 Suppose \gamma = 0.5 and the following sequence of rewards is received R1 = 1,
R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0, G1, ..., G5? Hint:
Work backwards. ⇤

G_0 = r_1 + \gamma G_{1}
G_1 = r_2 + \gamma G_{2}
G_2 = r_3 + \gamma G_{3}
G_3 = r_4 + \gamma G_{4}
G_4 = r_5 + \gamma G_{5}
G_5 = 0

G_4 = 2 + 0 = 2
G_3 = 3 + 0.5 * 2 = 4
G_2 = 6 + 0.5 * 4 = 8
G_1 = 2 + 0.5 * 8 = 6
G_0 = 1 + 0.5 * 6 = 4

Exercise 3.9 Suppose \gamma = 0.9 and the reward sequence is R1 = 2 followed by an infinite
sequence of 7s. What are G1 and G0? ⇤

$G_1 = 7 + \gamma G_2$

$G_2 = 7 \sum_{k=0}^{\infty} \gamma^k = \frac{7}{1 - \gamma} = 70$

$G_1 = 7 + 0.9*70 = 70$

$G_0 = 2 + 0.9*70 = 65$

Exercise 3.10 Prove the second equality in (3.10). ⇤

$G_0 = \sum_{k=0}^{\infty} \gamma^k$ is the geometric series.

$G_0 = \gamma^0 + \gamma^1 + \gamma^2 + \gamma^3 + ... + \gamma^\infty$

$G_0 = 1 + \gamma (1 + \gamma + \gamma^2 + ... + \gamma^\infty)$

$G_0 = 1 + \gamma G_0$

$G_0 = 1 + \gamma G_0$

$G_0 - \gamma G_0 = 1$

$G_0 (1 - \gamma) = 1$

$G_0 = 1 / (1 - \gamma)$

Exercise 3.11 If the current state is St, and actions are selected according to a stochastic
policy \pi, then what is the expectation of Rt+1 in terms of \pi and the four-argument
function p (3.2)? ⇤

Framing this as an expectation means we need to sum across all possible actions

$\sum_{a} \pi(a | S_t) \sum_{s', r} r * p(r| s', a)$

Exercise 3.12 Give an equation for v⇡ in terms of q⇡ and \pi. ⇤

$v_\pi(s) \doteq E_\pi[G_t | S_t = s]$

$q_\pi(s, a) \doteq E_\pi[G_t | S_t = s, A_t = a]$

---

To write in terms of q we just need to marginalize over all actions

$v_\pi(s) \doteq \sum_{a} \pi(a|s) E_\pi[G_t | S_t = s, A_t = a]$

that last term is the same as q

$v_\pi(s) \doteq \sum_{a} \pi(a|s) q_\pi(s, a)$

Exercise 3.13 Give an equation for q⇡ in terms of v⇡ and the four-argument p. ⇤

$q_\pi(s, a) \doteq E_\pi[G_t | S_t = s, A_t = a]$

Expanding out G_t

$q_\pi(s, a) \doteq E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$

Now we can condition on the next state to get v

$q_\pi(s, a) \doteq E_\pi[R_{t+1} + \gamma E[G_{t+1} | S_{t+1}] | S_t = s, A_t = a]$

$q_\pi(s, a) \doteq E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$

and now since we have something with the shape (s', r | s, a) we can undo the expectation using the 4 argument p

$q_\pi(s, a) \doteq \sum_{s'} \sum_{r} p(s', r | s, a) * [r + \gamma v_\pi(s')]$

Exercise 3.14 The Bellman equation (3.14) must hold for each state for the value function
v⇡ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds
for the center state, valued at +0.7, with respect to its four neighboring states, valued at
+2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.) ⇤

The four actions are equally likely, discount factor is 0.9

the discounted other rewards are 2.07, 0.36, 0.36, 0.63

$0.7 = \sum_{a} 1/4 \sum_{s, r} 1[r + \text{discounted reward}]$

$0.7 = \frac{1}{4} (0 + 2.07) + \frac{1}{4} (0 + 0.36) + \frac{1}{4} (0 + 0.36) + \frac{1}{4} (0 + 0.63)$

$0.7 = 0.5175 + .009 + .009 + .1575$

0.7 = 0.693 (accurate enough to the tenth)

Exercise 3.15 In the gridworld example, rewards are positive for goals, negative for
running into the edge of the world, and zero the rest of the time. Are the signs of these
rewards important, or only the intervals between them? Prove, using (3.8), that adding a
constant c to all the rewards adds a constant, vc, to the values of all states, and thus
does not affect the relative values of any states under any policies. What is vc in terms
of c and ? ⇤

Only the differences are important if we're trying to maximize it, the signs are mostly useful to semantically describe which are rewards and which are punishments. The advantage of a good state over a bad one exists independent of sign. 

$G_t \doteq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

$G_t \doteq \sum_{k=0}^{\infty} \gamma^k (R_{t+k+1} + c)$

$G_t \doteq \sum_{k=0}^{\infty} [\gamma^k R_{t+k+1} + \gamma^k c]$

$G_t \doteq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} + \sum_{k=0}^{\infty} \gamma^k c$

Since it's a constant term (i.e. a sum of constants) We can define $v_c = \sum_{k=0}^{\infty} \gamma^k c$ so $G_t \doteq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} + v_c$

Ergo, the relative value of the states will not change, because no matter what you will be adding $v_c$ to the state, which does not change from state to state.

Exercise 3.16 Now consider adding a constant c to all the rewards in an episodic task,
such as maze running. Would this have any e↵ect, or would it leave the task unchanged
as in the continuing task above? Why or why not? Give an example. ⇤

In an episodic task, it does cause problems to add a constant to all values. Consider maze running. If you have a negative reward for each non-solved turn, and then a big positive reward at the end, your total reward is maximized by getting out of the maze as fast as possible. If you have a small positive reward for each non-solved turn, and then an even bigger reward at the end, your total reward is now maximized by existing in the maze for all eternity, since eventually you will accumulate more reward by deliberately not finding the exit and bounding your reward. 

Exercise 3.17 What is the Bellman equation for action values, that
is, for q_\pi? It must give the action value q_\pi(s, a) in terms of the action
values, q_\i(s', a'), of possible successors to the state–action pair (s, a).
Hint: The backup diagram to the right corresponds to this equation.
Show the sequence of equations analogous to (3.14), but for action
values.

Well let's start from bellman equation for values

$v_\pi(s) \doteq \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]$

We've already shown we can write v in terms of q

$v_\pi(s) \doteq \sum_{a} \pi(a|s) q_\pi(s, a)$

so it seems to emerge that we can just do this

$q_\pi(s, a) \doteq \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]$

/o1: this might be wrong?/

Exercise 3.18 The value of a state depends on the values of the actions possible in that
state and on how likely each action is to be taken under the current policy. We can
think of this in terms of a small backup diagram rooted at the state and considering each
possible action:

Give the equation corresponding to this intuition and diagram for the value at the root
node, v⇡(s), in terms of the value at the expected leaf node, q⇡(s, a), given St = s. This
equation should include an expectation conditioned on following the policy, ⇡. Then give
a second equation in which the expected value is written out explicitly in terms of ⇡(a|s)
such that no expected value notation appears in the equation. ⇤

$v_\pi(s) \doteq \mathbb{E}[q_\pi(s, a) | s = S_t]$

$v_\pi(s) \doteq \sum_{a} \pi(a|s) q_\pi(s, a)$

Exercise 3.19 The value of an action, q⇡(s, a), depends on the expected next reward and
the expected sum of the remaining rewards. Again we can think of this in terms of a
small backup diagram, this one rooted at an action (state–action pair) and branching to
the possible next states:

Give the equation corresponding to this intuition and diagram for the action value,
q⇡(s, a), in terms of the expected next reward, Rt+1, and the expected next state value,
v⇡(St+1), given that St =s and At =a. This equation should include an expectation but
not one conditioned on following the policy. Then give a second equation, writing out the
expected value explicitly in terms of p(s', r|s, a) defined by (3.2), such that no expected
value notation appears in the equation. ⇤

$q_\pi(s, a) \doteq \mathbb{E}[R_{t+1} + \gamma v_\pi(s') | s = S_t, a = A_t]$

$q_\pi(s, a) \doteq \sum_{s', r} p(s' r | s, a) [r + \gamma v_\pi(s')]$ 

Exercise 3.20 Draw or describe the optimal state-value function for the golf example. ⇤

In the golf example the optimal state value function is $max_a \sum_{s', r} p(s' r | s, a)[r + \gamma max_a q_*(s', a')]$

As a result, the state-value function should look like the listed q*(s, driver) contours but with the values subtracted by 1, since the cost of the action is -1 

Exercise 3.21 Draw or describe the contours of the optimal action-value function for
putting, q⇤(s, putter), for the golf example. ⇤

it will have the same first contour as v_putt, but then it will have the contours of v_{driver}, until you get to the green, which will entirely be -1 (return to putting)

Exercise 3.22 Consider the continuing MDP shown to the
right. The only decision to be made is that in the top state,
where two actions are available, left and right. The numbers
show the rewards that are received deterministically after
each action. There are exactly two deterministic policies,
⇡left and ⇡right. What policy is optimal if \gamma = 0? If \gamma = 0.9?
If \gamma = 0.5? ⇤

if \gamma is zero, future rewards will be ignored, and you'll prefer \pi_{left} which provides immediate reward. With \gamma = 0.9, you'll prefer \pi_{right} since you'll care a lot about the resulting +2 after the first state. At \gamma = 0.5, both policies are equivalent, since left is $1 + 0.5(0) + 0.25 R_{t+3}$ and right is $0 + 0.5(2) + 0.25 R_{t+3}$. 

Exercise 3.23 Give the Bellman equation for q_* for the recycling robot. ⇤

Given that v_* is provided in the text, and v_*(s) = max q_*(s, a), we can just say

$q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$

where v_*(s') are the provided optimality equations for the recycling robot from the text.

I don't really want to write it all out in tex. I can revisit this if necessary.

Exercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as
24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express
this value symbolically, and then to compute it to three decimal places. ⇤

Recall the bellman equation

$v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s, r} p(s' r | s, a) [r + \gamma v_\pi(s')]$

In our case, we have a reward of 10, a fixed action, and a certain probability of identical reward and state transition. So:

$v_*(s) = 10 + \gamma v_*(s')$

We know that v_*(s') here is 16, and I think it was mentioned that \gamma was 0.9

Ergo $v_*(s) = 10 + 0.9(16) = 24.400$

A bit confused about this problem, I guess I could chain it together until I arrive back at v_* but I don't really feel like doing that at the moment.

Exercise 3.25 Give an equation for v_* in terms of q_*. ⇤

Isn't this just $v_* = max_a q_*(s, a)$

Exercise 3.26 Give an equation for q⇤ in terms of v⇤ and the four-argument p. ⇤

This was already in the text I think, it's $q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$

Exercise 3.27 Give an equation for \pi_* in terms of q_*. ⇤

$\pi_*(a | s) = \mathbb{1}[q_*(s, a) = max_{a \in A}(q_*(s, a))]$

Exercise 3.28 Give an equation for \pi_* in terms of v_* and the four-argument p. ⇤

$\pi_*(a | s) = \mathbb{1}[\sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] = max_{a}(\sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')])]$

Exercise 3.29 Rewrite the four Bellman equations for the four value functions (v_\pi, v_*, q_\pi,
and q_*) in terms of the three argument function p (3.4) and the two-argument function r
(3.5). ⇤

$v_\pi(s) = \sum_{a} \pi(a|s) [r(s, a) + \gamma \sum_{s'} p(s' | s, a) v_\pi(s')]$

$v_*(s) = max_{a \in A} [r(s,a) + \gamma \sum_{s'} p(s'|s, a) v_*(s')]$

$q_\pi(s, a) = r(s, a) + \gamma \sum_{s'} p(s' | s, a) \sum_{a'} \pi(a' | s') q_\pi(s', a')$

$q_*(s, a) = r(s, a) + \gamma \sum_{s'} p(s' | s,a) max_{a'}q_*(s', a')$

/status: kinda rocky, but checked with o1-preview/

*** 4

Exercise 4.1 In Example 4.1, if \pi is the equiprobable random policy, what is q_{\pi}(11, down)?
What is q_{\pi}(7, down)?

Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below
state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14,
and 15, respectively. Assume that the transitions from the original states are unchanged.
What, then, is v_{\pi}(15) for the equiprobable random policy? Now suppose the dynamics of
state 13 are also changed, such that action down from state 13 takes the agent to the new
state 15. What is v_{\pi}(15) for the equiprobable random policy in this case? ⇤

Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5), but for actionvalue functions instead of state-value functions?

Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may
never terminate if the policy continually switches between two or more policies that are
equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode
so that convergence is guaranteed. ⇤

Exercise 4.5 How would policy iteration be defined for action values? Give a complete
algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay
special attention to this exercise, because the ideas involved will be used throughout the
rest of the book. ⇤

Exercise 4.6 Suppose you are restricted to considering only policies that are "-soft,
meaning that the probability of selecting each action in each state, s, is at least "/|A(s)|.
Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1,
in that order, of the policy iteration algorithm for v⇤ on page 80. ⇤

Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car
rental problem with the following changes. One of Jack’s employees at the first location
rides a bus home each night and lives near the second location. She is happy to shuttle
one car to the second location for free. Each additional car still costs $2, as do all cars
moved in the other direction. In addition, Jack has limited parking space at each location.
If more than 10 cars are kept overnight at a location (after any moving of cars), then an
additional cost of $4 must be incurred to use a second parking lot (independent of how
many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often
occur in real problems and cannot easily be handled by optimization methods other than
dynamic programming. To check your program, first replicate the results given for the
original problem.

Exercise 4.8 Why does the optimal
policy for the gambler’s problem have such a curious form? In particular, for capital of 50
it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy? ⇤

Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and
solve it for ph = 0.25 and ph = 0.55. In programming, you may find it convenient to
introduce two dummy states corresponding to termination with capital of 0 and 100,
giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3.
Are your results stable as ✓ ! 0? ⇤

Exercise 4.10 What is the analog of the value iteration update (4.10) for action values,
qk+1(s, a)? ⇤
