#+TITLE: Model Merging

* Model Merging and You

Model Merging is a weird and experimental technique which lets you take two models and combine them together to get a new model. This is primarily used in Large Language Models, where the likely convergent representations allow this technique to work somewhat better than you might expect, given the concept. Model merging techniques are interesting since they allow researchers to create new models which are somehow performant in new ways despite not doing any additional training. I think this is really weird, so I need to know about it.

There is some work applying these sorts of merges to vision models, too, like Stable Diffusion. People will often train several adapters (e.g. one for style, one for subject, etc) and then combine them all upon the base model to get something which is performant on all the sub-tasks that were trained. We are going to read a lot of papers and try to get some sense for why this works. 

There are many good [[https://huggingface.co/blog/mlabonne/merge-models][resources]] out there for understanding how to apply mergekit to large language models. I am interested in building some intuition upon how and why these methods work, and specifically if there's any room for applying these techniques to other types of models, or other problems.

** Actually Doing The Merging

[[https://github.com/arcee-ai/mergekit?tab=readme-ov-file#merge-methods][mergekit]] is popular for this. It has a bunch of different methods.

** Papers

[[https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66][Model Merging Collections]]

Here's a [[https://arxiv.org/pdf/2309.15698][survey paper]]

** Convergent Learning: Do Different Neural Networks Learn the Same Representations?

Paper can be found [[https://arxiv.org/abs/1511.07543][here]]. This is an old paper from 2016, but we're starting with it because it was an early paper which explored the idea that two independent networks trained on similar data could converge upon similar features, even if they happen to be permuted.

This paper trained a number of AlexNet models on ImageNet data, and then compared how similar the filters were to the closest matches in the corresponding other model. They make 5 main observations:

1. The core representation is the same, but rare features sometimes appear in one model and not the other.
2. One-to-one alignment of neurons holds even if you use different measures of similarity (i.e. it's not some weird artifact of the metric)
3. We can create a transformation which converts one model's representation into the other's (!!!)
4. We can see many-to-many mappings between clusters of neurons as well, on top of just at the neuron level
5. The activation statistics between matching neurons are mostly similar as well

To run these experiments, they do multiple training runs with the same architecture on the same data. This makes it pretty likely that the features it would learn would be roughly the same, but in different spots due to random initialization. They then use bipartite semi-matching[fn:2] to pair each filter with it's most likely counterpart in the network. A good number of these are almost perfect matches, but the worst matches suggest that each network has some set of "rare features" that it learned, departing from the other network[fn:1]. 

[[../images/from_clipboard/20240731_101104.png]]

To really show how the two are related, they learn a single "mapping" layer (a LASSO model, with an L1 penalty) which will predict one network's representation when provided the other. They can use the sparse prediction loss for this layer as a metric for how similar the representations are (i.e. if the loss is very low, the representations are likely to be more like a permutation, but if it's high there's probably lots of different features which don't match). 

[[../images/from_clipboard/20240731_102448.png]]

There's some good discussion here about /local/ vs /distributed/ representations, i.e. ones which can be directly matched one-to-one and ones which are still easy to predict by the mapping layer despite not mapping one-to-one. The result here is interesting because it suggests a pretty high degree of local mapping, suggesting that the neurons are learning important specific vectors individually rather than just being a collection of essentially random basis vectors which describe some specific subspace[fn:3]. 

Okay: how does this help us with model merging? Here's some takeaways:

- Similar features learned between networks trained on similar / the same task
- Possible to create a mapping transforming one model's representation to the other's
- From the future work section: "Model Combination: Can multipled models be combined by concatenating their featurs, deleting those with high overlap, and then fine-tuning?"

Also probably the biggest takeaway: these models are start from unique initializations, and as a result of the random initialization they don't share the same initial optimization trajectories, meaning all of these shared features are somewhere in the model but in a random arrangement. If we finetune some sort of source model and then "break off" from there, it's really likely that the features are all going to be in the same spots, with the differences then being about the subsequent differences from different data / hyperparameters / whatever. Indeed this was exactly what was later shown in [[https://arxiv.org/pdf/1912.05671][Linear Mode Connectivity and the Lottery Ticket Hypothesis]]. This informs a lot of the methods which follow from here!

** (SLERP) Sampling Generative Networks

Paper can be found [[https://arxiv.org/pdf/1609.04468][here]]. This paper is also an older one, from 2016.

This paper is specifically about sampling from generative networks, but the core idea of the paper was later applied to model merging. Say you want to sample between two points from a generative model (e.g. interpolate between a human and a tiger). One naive thing you could do is simple linear interpolation, where you take the weights which generated the first image, the weights that generated the second image, and take a straight line between these two points and sample along the way at regular intervals. This will, in principle, get you a series of images which start and end with your images.

However, latent spaces are high dimensional, and traveling along linear paths like this are extremely unlikely given the gaussian / uniform priors these models typically have. What would be better if you could interpolate along the [[https://en.wikipedia.org/wiki/Geodesic][geodesic]] in that high-dimensional space, which is more in line with what you want (a "straight line" between both points -- in a curved space this isn't a linear interpolation).

They propose using /SLERP/, which is a spherical linear interpolation along an n-dimensional hypersphere. This seems to do better.

[[../images/from_clipboard/20240731_110449.png]]

Stable Diffusion finetunes are commonly SLERPed together like this -- often the more popular models aren't base model but rather models with a bunch of these community finetunes SLERPED on top of each other in the weight space[fn:4]. 

** Model Soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time

Paper can be found [[https://arxiv.org/abs/2203.05482][here]]. This paper is from 2022, and was one of the early modern "model merging" works[fn:5].

The core idea here is that if instead of selecting the best-performing model training run, or making an ensemble from all the trained models, we can just literally average together the weights and get the benefits of an ensemble with only one model's worth of inference and memory costs. This is a pretty interesting result: the averaged weights of multiple models yield a model which outperforms any individual model! They used this to break state-of-the-art on ImageNet. Core to this is that they're all the same architecture and trained on the same data (just with varying hyperparameters).

A lot of this relies on an [[https://arxiv.org/pdf/2008.11687][earlier result]] which showed that "fine-tuned models optimized independently from the same pre-trained initialization lie in the same basin of the error landscape". There was also some earlier work averaging the weights of a single model along a single training run (maintaining a sort of moving average), but this work extended it to combining several models trained independently. They actually show this in figure 2 directly:

[[../images/from_clipboard/20240731_131554.png]]

Specifically they find that using held-out data and taking a greedy approach to averaging in new models (adding them if they increase performance on held-out data) is the best way to do this. They find that this seems slightly worse than ensembles on test, and slightly better than ensembles under distribution shift. They use CLIP ViT-L, ViT-G, ALIGN (uses efficientnet for vision encoder), BASIC, a big variety of models of different types.

The related work has some interesting caveats: [[https://arxiv.org/pdf/1912.05671][Linear Mode Connectivity and the Lottery Ticket Hypothesis]] showed different data order makes simple weight averaging ineffective (i.e. equivalent to random), but that if the two models share some part of their optimization trajectory, that suddenly averaging them works well again. In [[https://arxiv.org/pdf/2008.11687][What is being transferred in transfer learning?]] it was shown that interpolating between two finetunes will have at least the accuracy of the endpoints. 

** Merging Models with Fisher-Weighted Averaging

The paper can be found [[https://arxiv.org/pdf/2111.09832][here]]. 

Up to this point, averaging models' weights together is generally only feasible if the models share initialization and architecture. This paper frames the model merging process as approximately maximizing the joint likelihood of the models' posterior distribution over parameters, and that if you use an isotropic gaussian distribution to approximate the posterior, maximizing this joint likelihood is equivalent to just averaging the weights. They call this /isotropic merging/ as a result.

In contrast, they think a better way to approximate this posterior would yield a better result. So, use the Laplace approximation instead, by taking the diagonal of each model's [[https://en.wikipedia.org/wiki/Fisher_information][Fisher information matrix]] as the precision matrix for that model's posterior. They call this /Fisher merging/ to distinguish it from isotropic merging, and they show that it's often a bit better.

[[../images/from_clipboard/20240731_173548.png]]

[[../images/from_clipboard/20240731_173631.png]]

The difference here is pretty subtle, but it does seem a little bit better than normal averaging[fn:9]. 

** Editing Models with Task Arithmetic

Paper can be found [[https://arxiv.org/pdf/2212.04089][here]]. 

/Task Arithmetic/ builds task vectors by subtracting pre-trained weights from fine-tuned model weights. What you get as a result is a vector where if you apply it to the base model, you improve it at that task. If you build a bunch of task vectors, you can do interesting vector arithmetic with them: negating the vector will make you worse at that task, adding task vectors together will make your model better at both things, etc. You can even improve performance through task analogies, e.g. /A is to B as C is to D/, where adding A, B, and C to the model as task vectors will improve D even with no data or training directly on that task.

This is interesting because we can /remove/ things by training models which /do/ those things. For example, if we train a toxic model and then add the negated toxic task vector, we get a less toxic model. We can /learn via addition/ or /forget via negation/. Task analogies work a similar way: for example, we can approximate a task vector for "Yelp Sentiment Classification" by starting from "Amazon Sentiment Classification", adding "Yelp Language Modeling" and subtracting "Amazon Language Modeling". 

[[../images/from_clipboard/20240731_145513.png]]

The above is essentially the entire content of the paper, it's very simple. The rest after this figure is formalization ($\theta_{new} = \theta + \lambda\tau$ where $\tau = \theta_{ft} - \theta_{pre}$, and this is equivalent to a full finetune when $\lambda = 1$) and experiments on a variety of image and natural language processing models/tasks.

The discussion section has a lot of really interesting points. One big finding they see is that vectors from different tasks are close to orthogonal, which is what you would expect if the different tasks are essentially random vectors (which are likely to be close to orthogonal in high dimension). This likely helps explain why adding them together seems to cause minimal interference with each task. Likewise, intermediate task vectors seem to converge very quickly to the appropriate direction, suggesting that you could even potentially do crazy things like halt training early and just modify the magnitude of the task vector instead. They also reference the [[https://arxiv.org/pdf/2209.04836][git re-basin]] paper as potential work where the merging could occur between models which are not derivatives of the same base model.

Overall this seems like a promising merging direction, and in general seems like a cool step towards making models more generally interpretable in the first place. One could imagine a model with tons of these little task vectors applied to it, where you can visibly modify specific behaviors this way. 

** TIES-MERGING

The paper can be found [[https://papers.nips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf][here]]. 

Existing merging methods tend to ignore interference between parameters of different models, and this is what the authors claim is the source of performance drops during merges. The two major sources of said interference are 1: redundant parameter values, and 2: disagreement on the sign of a parameter's value.

TIES-MERGING stands for... TrIm, Elect Sign and MERGE[fn:6]. This, appropriately, has three steps. First, clip parameters that only changed a little bit during training. Second, resolve the sign conflicts. Third, merge only the parameters that are in alignment with agreed-upon sign. This seems to help!

[[../images/from_clipboard/20240731_154456.png]]

[[../images/from_clipboard/20240731_155253.png]]

This is considered one of the more sophisticated methods despite still being just a pretty simple modification to task arithmetic. This outperforms vanilla Task Arithmetic, RegMean, Fisher Merging, and Model Soups, but obviously it doesn't really do anything different from task arithmetic if you're only merging one task vector to the base model.

Why does this work? Don't we need the little updates too, given that the gradient updates we got from training produced them? Turns out no, you really don't -- most of the difference in performance comes from the parameter changes which are really big, and literally zeroing out 80% of the task vector will usually do almost nothing to the performance.

[[../images/from_clipboard/20240731_161638.png]]

So it's empirically well-motivated[fn:7] to trim out the activations which are small, leaving us a task vector which is mostly sparse and mostly does the same thing, but is less likely to cause problems with the model merge process, especially if the values would cause sign disagreements.

For sign disagreements, they pick the one with the highest total magnitude across all the models (i.e. sum of all the + values vs sum of all the - values). They "disjoint merge" means you set everything which is the wrong sign to 0, and then from there it's a normal merge[fn:8]. This seems to perform pretty well, usually outperforming other methods on most tasks, and performing worse if any of the steps are ablated (i.e. making it more similar to vanilla task arithmetic).

** (DARE) Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch

This paper can be found [[https://arxiv.org/pdf/2311.03099][here]]. The framing of this paper is EXTREMELY funny. Language models are Super Mario! You know, because they absorb, uh, items.

DARE is another method which zeros out small differences, it stands for Drop And REscale. This is often combined with other methods in practice. The step which most differentiates this from TIES-MERGING is this final rescaling step -- on top of dropping parameters, they also scale the remaining ones by $1 / (1 - p)$ where $p$ is the random drop rate. With this addition, they find they're able to drop 90-99% of the delta parameters, which means you can add lots of different vectors for very minimal cost. This paper, relative to other ones we've seen, is pretty explicitly only about language models, so it's unclear if this holds for all types of models.

[[../images/from_clipboard/20240731_165605.png]]

This has the most unnavigable figure I've ever seen in a paper, ever. Check this out:

[[../images/from_clipboard/20240731_165912.png]]

This tolerance depends on the size of the language model, i.e. one with a ton of parameters can withstand up to a 99% drop rate. Notably this is a /random/ drop, not a top-k drop as seen in TIES-MERGE. This makes the scaling factor really important, because without the highest magnitude features (which are most likely dropped), we very likely need to scale whatever parameters are left by a large value to the task vector roughly the same magnitude.

The delta pruning operation is not very novel but the real contribution of this paper is the comparison of this random drop strategy with the more common magnitude-based pruning. They find that if you rescale the non-dropped parameters, the random drop does much better, and you can drop even more parameters than you would be able to with magnitude-based pruning. This is sort of counterintuitive, but it's seemingly because some signal actually does exist in the small activations after all.

This is definitely the most hacky of the papers so far -- there's even a whole section on if this works if you drop the entire fine-tuned parameter instead of the delta (it, uh, doesn't work). But definitely an interesting takeaway that, at least for language problems, pruning the task vectors randomly and rescaling might be a better try than pruning based on magnitude.  

** Evolutionary Optimization of Model Merging Recipes

[[https://arxiv.org/pdf/2403.13187][Paper]]. [[https://github.com/SakanaAI/evolutionary-model-merge][code]]. Sakana AI's thing, which is exciting because they used it on a diffusion model and it worked well. 

The central claim of this paper is that model merging techniques are cost-effective and promising, but rely on human intuition and domain knowledge to perform well. To get around this, they do a bunch of stuff to automatically discover ths best way to combine models.

This work is extra significant because it features Cross-Domain Merging, i.e. it's a model merging technique which can merge models even if they aren't just two models trained to do the exact same thing on the same data, with minor differences (e.g. what a lot of people think makes merging work for LLMs)

The last technique is /Frankenmerging/ which just puts different layers from different models into one model. Who knows how this works, or if it's useful for non language problems.

The evo work is like NAS but using transformer blocks instead of neurons, and doesn't need to train the model (it's usable immediately). Layer stacking models do not have the constraint where they need to come from the same base model, which is an interesting property

They merge both in parameter space (combine layers together) and data flow space (connect blocks together in different ways). 

Maybe some potential here for a funny mixture of agents thing?

** Appendix: Other Papers

[[https://arxiv.org/pdf/2204.03044][fusing finetuned models for better pretraining]] 2022 model averaging paper: averaging different fine tunes is better than using pretrained models.

[[https://openreview.net/pdf?id=FCnohuR6AnM][dataless knowledge fusion by merging weights of language models]] 2023 ICLR paper, RegMean, sort of a combination of fisher merging and simple linear merging, which minimizes l2 distance to individual model predictions on the training sets.

Methods for finding permutations to merge unrelated models:

[[https://arxiv.org/abs/2209.04836][git re-basin]] mering models modulo permuation symmetries. I'm definitely going to read this and graduate it to the main part of this but I'm scared.

[[https://arxiv.org/pdf/1910.05653][Model Fusion via Optimal Transport]]

[[https://arxiv.org/pdf/2002.06440][Federated Learning with Matched Averaging]]

* Footnotes

[fn:9] I don't have that much to say on this paper -- it's important and gets brought up a lot, but it's mostly just a slightly more interesting averaging vs normal averaging. 

[fn:8] Maybe worth noting: the averaging process ignores zeros, both from trimmed vectors and from sign-election. This method wouldn't be worth much if setting the values to 0 could drag the average towards 0. 

[fn:7] I think theoretically it seems strange to me that these little values don't do anything but can't argue with a figure like that I suppose. Wonder if it's task-dependent.

[fn:6] I feel like we just get worse at naming as time goes on.

[fn:5] They really lean into this "soup" analogy

[[../images/from_clipboard/20240731_112528.png]]

[fn:4] Need to find an example of this, this was just something mentioned in the Sakana AI Evo-merging paper.

[fn:3] I vaguely remember some paper from a long time ago about permuting the weights of a neural network and still doing well, potentially related. Could just be making this up, though, since I can't find it now.

[fn:2] That is, you can match multiple filters to the same filter. More useful than strict matching because if you have e.g. 6 filters for faces in network A and 5 filters for faces in network B, it's annoying to match the left-out filter from network A to some random filter elsewhere.

[fn:1] Very interesting: Does seem to suggest that there are useful features left to be learned for each network. Intuitively feels like an ensemble of nearly identical networks could somehow be useful if you could somehow "trim out" the shared core between the two of them. 
