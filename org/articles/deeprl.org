#+TITLE: Deep RL

* Deep RL Notes

I have managed to complete Sutton and Barto's /[[https://planetbanatt.net/articles/sutton.html][Reinforcement Learning]]/. I would like to now learn enough deep reinforcement learning to do some useful, novel things with it.

A natural extension step here is OpenAI's /Spinning Up in Deep Reinforcement Learning/ from 2020. This is mostly review for me except for the important subsection [[https://spinningup.openai.com/en/latest/spinningup/keypapers.html][Key Papers in Deep RL]] which contains roughly 100 papers in deep RL. Likewise, there's a nice library called [[https://docs.cleanrl.dev/][CleanRL]] which contains single-file, minimal implementations of popular deep RL algorithms for pedagogy.  

This page is going to contain two things. It will contain notes on code, and notes on papers. These won't necessarily be from the 100 papers in spinning up / from cleanRL, but a lot of them will be. I'll be reading these with varying level of depth spent on each papers, which I will note how closely I've read each paper.

I'm going to try to read at least a couple papers each day in deep RL. 

** Papers

*** Playing Atari with Deep Reinforcement Learning (DQN)

Original DQN paper from 2013, very short (there's a longer Nature version which was released later)

Notable about this paper is the fact that all the games are deterministic, so they can formalize an MDP by having the state be the complete list of observations+actions starting from the starting state. I think this is kind of silly, since there are sure to be duplicate states this way, and it throws away most of the point of the markov property, but I guess to theoretically justify making it work for RL it's fine, it's mostly just for completeness.

[[../images/from_clipboard/20241029_111102.png]]

Basically, generate a bunch of episodes and store memories of all your states, and rather than updating your single episode just update on a random minibatch drawn from the states in your dataset.

The claim here is that nonlinear function-approximation Q-learning is likely to diverge because of correlations between consecutive states, and that doing random updates on the last N states is likely to smooth this out and make it work again. In a sense the "Actual Contribution" of this paper is Experience Replay, the rest is just engineering details like using a CNN to understand the frame / using frame skip to make episodes shorter / doing mostly standard Q learning.

*** Asynchronous Methods for Deep Reinforcement Learning (A3C)

found [[https://arxiv.org/abs/1602.01783][here]], from 2016.

The core insight in this paper seems to be this idea of replacing experience replay with multi-threaded episodes running in parallel. This means that the updates are sort of pseudorandom still, since it's likely that multiple agents running async will be in different spots of the environment at any given time.

The main advantage of doing this is that now we can stablize on-policy methods! Since before off-policy was the only type that would work with experience replay, and without experience replay it was unlikely to be stable with deep methods. The asynchrony is sort of like a faux experience replay since you get updates all over the state space from the multiple runs, but you get to stay on-policy.

The asynchronous advantage actor-critic (A3C) algorithm in this paper maintains a policy and a value estimation function (as is done in actor-critic), and updates based on $\nabla_{\theta^{'}} log \pi(a_t|s_t, \theta') A(s_t, a_t; \theta, \theta_v)$ where A is the advantage function
$A(s_t, a_t; \theta, \theta_v) = \sum_{i=0}^{k-1}\gamma^i r_{t+i} + \gamma^kV(s_{t+k}; \theta_v) - V(s_t; \theta_v)$.

Running this does much better than DQN on atari, ~6x better and ~20x faster to reach DQN performance.

*** Continuous Control with Deep Reinforcement Learning (DDPG)

A notable limitation with DQN is low-dim and discrete action spaces -- it will fail if either of those is changed.

This paper introduces Deep Deterministic Policy Gradient (DDPG) which adds the experience replay / target network stuff from DQN (along with batch norm) to DPG. It also introduced the "soft update" for the target network, which updates the weights via exponential moving average rather than all at once.

[[../images/from_clipboard/20241102_173841.png]]

The core thing here is using actor-critic to extend DQN to continuous action spaces. The DQN algorithm relies on being able to calculate $\gamma max_a Q(S_{t+1}, a)$ which cannot be performed if there are an infinite number of actions available. However, the actor-critic formulation takes care of both of these, so it's a fairly natural extension.

They show it working well on a bunch of mujoco environments. 

*** High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE)

Paper [[https://arxiv.org/pdf/1506.02438][here]], from 2015. Two paragraph abstract is kind of a fun curiosity.

This paper is like TD(\lambda) for policy gradient / online actor-critic methods. The goal is to reduce variance without adding too much bias, since the high variance prevents previously existing policy gradient methods from learning hard tasks, and high bias is what prevents these methods from converging in the first place.

The primary contribution of this paper is laying out the Generalized Advantage Estimator, which is an advantage function which is discounted ($\gamma$) and also uses an eligibility trace ($\lambda$). The derivation is the same telescoping sum of $\delta$ trick that we had to do like 200 times in Sutton and Barto, so it should be easy to get through for those among us who fought through the exercises.

[[../images/from_clipboard/20241102_225433.png]]
[[../images/from_clipboard/20241102_225444.png]]
[[../images/from_clipboard/20241102_225504.png]]

that is, it's pretty much Exercise 12.3 in Sutton and Barto, but instead of using the value function (as in TD(\lambda)), it uses the advantage function ($Q(s_t, a_t) - V(s_t)$).

Lots of theory in this paper which are mostly theoretical fun facts. We can interpret \lambda in the above as a discounted reward shaping applied to the MDP. Setting \lambda = 0 has lower variance but introduces bias, \lambda = 1 has no bias but high variance. As a result, moving it around can be thought of as navigating a bias-variance tradeoff.

Estimating the value function here is treated like a regular regression problem, and solved with a "trust region method" where they minimize $\sum_{n=1}^{N}||V_\phi(s_n) - \hat{V}_n||^2$ subject to $\frac{1}{N} \sum_{n=1}^{N} \frac{||V_\phi(s_n) - \hat{V}_n||^2}{2\sigma^2} \leq \epsilon$. (i.e. average KL divergence between values functions is less than epsilon).

They applied this variant of advantage function to Trust Region Policy Optimization (TRPO) on various tasks and found that it made them much more useful for more difficult control tasks (e.g. quadrupedal walking).

*** Proximal Policy Optimization Algorithms (PPO)

Paper [[https://arxiv.org/abs/1707.06347][here]]. You could call this, vaguely, sota for deep RL algorithms; it's what was used for stuff like OpenAI Five, it's extremely powerful.

The actual PPO algorithm is super simple:

[[../images/from_clipboard/20241102_210014.png]]

The important things here are the surrograte objective and the advantage estimates. For the former, the one that matters most is the clipped surrogate objective, which takes the following form:

$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t [min(r_t(\theta)\hat{A}_t, clip(r_t, 1-\epsilon, 1+\epsilon) \hat{A}_t)]$

Where $r_t$ is the probability ratio $\frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$.

The first term of this clipped objective is the same objective as Trust Region Policy Optimization. The intuition behind this that we have a metric for how much better or worse than expected our action was (the advantage). We also have the old and new policy, and we can measure how much more or less likely either policy was to take that action. In short, if the action was better than expected, we want our new policy to take that action more than the old policy. If the action was worse than expected, we want our old policy to have taken that action more frequently than the new policy. In effect, maximizing this objective means that we directly try to make advantageous actions more likely under the new policy.

The second part of this clips the value of the probability ratio to be bounded within a specific range. The intuition behind this is that if we optimize the TRPO objective directly, we will want to make a huge update whenever there's a large advantage. But sometimes you'll just get that by chance, for example if the environment is stochastic. What we really want is to make steady changes to the policy to maximize the objective, and hope that through experience we can continue to improve it.

Then we take the minimum of this applied to the advantage function. The probability ratio is going to be positive (it's a ratio of probabilities), so basically this will clip the ratio to 1+\epsilon if the advantage is positive, and clip the ratio to 1-\epsilon if the advantage is negative. This lets us constrain to smaller updates, so that if we want to make big changes, we need to do it over multiple iterations through multiple experiences, rather than all at once (which could be unstable)

The other big component of this is the advantage estimates. This is done via a truncated Generalized Advantage Estimation (GAE), specifically:

$\hat{A}_t = \delta_t + (\gamma\lambda)\delta_{t+1} + ... + (\gamma\lambda)^{T-t+1}\delta_{T-1}$

Where $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$

Since it's an actor-critic method, we have the policy (actor) which makes all the moves, optimized via the surrogate objective, and we have a value model (critic) which is just trained to predict returns from each state (and therefore minimize $(V_\theta(s_t) - V_{target})^2$. In their case they put this all in one network with two heads, and use a combined loss function which optimizes both of these tasks at once. 

Compared to other deep RL algorithms like DQN, A3C, DDPG, etc; PPO is a lot more engineerish. It's extremely simple algorithmically, and doesn't use anything like replay buffers, optimization constraints, replay buffers. It works on continuous and discrete action spaces, it's easy to work in with other networks, and it works super well. But, in exchange, it's on-policy, and the clipping is just something that seems to work rather than something with more concrete theoretical guarantees.

There are some other objectives explored in the paper here but the clipped one is the one which has remained the most important to my knowledge. As mentioned, this method solves DoTA, so you don't need too many other bells and whistles beyond this.

*** Dota 2 with Large Scale Deep Reinforcement Learning (OpenAI Five)

Paper [[https://arxiv.org/pdf/1912.06680][here]]. OpenAI defeats world champion DoTA 2 team using a deep RL system trained via PPO.

The majority of this paper deals with the necessary architectural challenges present when scaling to a system large enough to solve such a difficult task. The actual training component is relatively straightforward, but training for months on 1000+ GPUs for a game which recieves patches and pulls the environment out from underneath you introduces significant challenges compared to something simple which can be run on a single machine.

[[../images/from_clipboard/20241103_114232.png]]

They train this such that each hero uses the same network, and the actor-critic component is handled by two heads of the same network (compare to the CleanRL PPO cartpole implementation which uses two separate networks). The LSTM layer is the big thing, it's ~84% of the parameters. Also note that this system is not from pixels -- they use an encoder function which maps game state protobufs to a set of observations which are deemed fair for the agent to observe (I don't play DoTA so I don't have too much of an opinion on which observations are fair or not, maybe something to revisit later).

[[../images/from_clipboard/20241103_114809.png]]

The system design here uses four primary types of machines: controller machines which hold and distribute new parameters after optimization updates, forward pass GPUs which calculate the actions to be taken, rollout workers which play the game (sending states to forward pass GPUs, applying actions from those GPUs to the environment, and sending samples to the optimizer machines), and optimizer GPUs which take the samples generated by the rollout worker and perform gradient updates.

They used this system for ten months on a single training run, and then it defeated the world champions.

Of interest to me from this paper is the /Surgery/ technique that they describe in Appendix B. They have an experiment in the main body which shows a "rerun" experiment where they use the final hyperparams / environment / architecture / etc and destroy their original performance in just 2 months of training. You might ask what the point of the old way was if you could just do this. The key insight is two main things:

1. DoTA 2 is regularly patched, which means the environment changes frequently, which would require completely retraining a model without some sort of trick.
2. They can demonstrate success with a scaled down experiment, and then scale that experiment up, rather than hyperparameter searching directly upon a huge and extremely expensive training run -- The rerun experiment is sort of like comparing figuring out a rubik's cube vs speedsolving a rubik's cube using a solution you already have, so it's not much of a shock that it performs better than the 10-month training run.

So how does this work? Specifically what they want to do is to handle these two things: changes to the architecture, and changes to the environment. Basically, we want to figure out how to put a policy into a bigger policy's "body" while having it act the same, and we want to show the model new or different things but have it act basically the same if it just ignored the new things.

For the first of these, they show a simple theoretical result where this will work simply by adding new random parameters at the end of the first layer (so there's no symmetry), and adding the same number of zeros to the layers that follow (so those random parameters are ignored at first). This won't work for the LSTM since it's a recurrent network, but they mention in practice they can just use very small random initialization and it works pretty much the same. This works for changing the observation space too: if you pad the weight of the matrix which consumes the observations with new zeroes then $\hat{\pi}_{\hat{\theta}}(\hat{E}(s)) = \pi_\theta(E(s))$ for all states $s$, and these just get changed via regular learning in future updates.

For changes to the environment, they show that there's usually some transfer if the observation space doesn't change (i.e. if you do nothing and just change the environment the agent will learn that the changes happened and then adjust accordingly), but they do mention that using annealing of the new environment helped stability. That is, you start with 0% of the new environment, and slowly replace proportions of your samples with ones from the new environment until you land at 100%. They do this so that the model never gets worse in skill due to the environment change, and if they notice the model getting worse during this process they conclude it's because they annealed too fast (and start over with a slower annealing rate).

Notable about surgery is that it never touches the reward (and thereby the behaviors), and it also does not work for /removing/ parts of the model. The architecture change mentioned above only works in one direction, so if you add an observation and later deprecate it, you have to leave it in the network as a vestigial component forever. They mention the Rubik's Cube Hand paper as something which tackles this question, but they just continued to observe useless observations instead of extending surgery techniques to support removals. 

*** Grandmaster level in StarCraft II using multi-agent reinforcement learning (AlphaStar)

Paper [[https://www.seas.upenn.edu/~cis520/papers/RL_for_starcraft.pdf][here]]. Using largely the same architecture as OpenAI Five, but with imitation-learning-driven RL instead of entirely self-play, DeepMind trains an agent which gets GM level at StarCraft II. There's a thesis from one of the first authors which goes into more detail [[https://uwspace.uwaterloo.ca/items/a01fd348-ae31-4a81-8164-3c7314fdfe09][here]] which I haven't read through yet.

Training for this was done in two large phases: first a supervised learning phase, and second a reinforcement learning phase.

The SL model was trained on a big corpus of 971k replays of StarCraft II matches for players with ~top 20% rating. For each action, they compute a distribution of possible outputs that the model can output as actions, and they train to minimize the KL divergence between this distribution and the human distribution. This was then further finetuned with only replays from top players, to encourage the action distribution to more closely match higher skill play. 

The RL model was trained second since the action space was so insanely large that it needed a human-centric starting point for action selection. They do actor-critic like in OpenAI Five, but with some slight differences. There are a lot more actions in StarCraft, so the time horizon and action space both pose really big problems.

The value function here was trained with TD(\lambda), in hopes of handling the longer time horizon. The policy was trained with V-Trace and Upgoing Policy Updates (UPGO). V-Trace comes from the [[https://arxiv.org/pdf/1802.01561v3][IMPALA]] paper (need to understand this better later). UPGO I think is an original contribution here. The basic idea here is that if the action taken by the policy was better than the value estimate, you learn from that, but if it's worse than the value estimate, you bootstrap from the value estimate instead. This makes it so that the policy is "always learning from good trajectories and ignoring bad ones".

A core component of starcraft is that there are three races. To be good at starcraft, you need to be good against all the races. This introduces a multi-agent problem which is maybe the most technically interesting component of this paper:

[[../images/from_clipboard/20241103_155450.png]]

The idea here is that you have three different types of agents: main agents (i.e. the thing we are training), exploiter agents (who only play main agents and thus learn to specifically exploit their weaknesses), and league exploiters (trained similar to main agent, but never have to play exploiters). There is one main agent for each race, and two main exploiter and league exploiter agents for each race, for a total of 15 different types of agents being trained at once.

Why can't they use self-play for this type of thing? Well for one, self-play collapses a bit in these sorts of rock-paper-scissors situations, where they chase in a circle learning strategies which defeat the currently held one. Better to have a population of strategies, and improve at defeating them all. Better yet, they select opponents based on their likely ability to defeat the agent (PFSP), such that almost no games are played between two agents where victory is roughly a sure-thing. The relative balance of PFSP vs normal self-play, how many agents of each type are in the pool, etc are all mentioned in the appendix, giving me the impression that the balance is a bit of an art. 

*** Successor Features for Transfer in Reinforcement Learning

Found [[https://arxiv.org/pdf/1606.05312][here]], from 2018.

This paper studies transfer between tasks in RL contexts.

The formulation here is really elegant. The basic idea is that we can create a model which will calculate expected reward on a task from features, i.e.

$r(s, a, s') = \phi(s, a, s')^\intercal w$

where $\phi(s, a, s')$ describes features, and $w$ represents weights which produce the reward given the features. This way, instead of writing Q(s,a) as the expected discounted sum of rewards, we can write it as the expected discounted sum of features, multiplied by the weights $w$.

[[../images/from_clipboard/20241031_131631.png]]

$\psi^\pi(s,a)$ here is the expected discounted sum of features, which they call /successor features/.

How do we compute this? Well, we can use any RL method, since it can just be written as a bellman equation where we substitute $\phi$ for rewards:

$\psi^\pi(s,a) = \phi_{t+1} + \gamma E^\pi[\psi^\pi(S_{t+1}, \pi(S_{t+1})) | S_t = s, A_t = a]$

WHY THIS IS USEFUL: because in cases where we keep everything the same EXCEPT the reward function, we can completely describe the difference between tasks as the difference in $w$.

WHY THIS IS USEFUL 2: If you have a set of policies $\pi_i$ and their successor features $\psi^{\pi_i}$, if you're given a new task $w_{i+1}$ then you can easily evaluate all the policies just by doing $\psi^{\pi_i}(s,a)^\intercal w_{n+1}$. You can then do /generalized policy improvement/ to construct a new policy based on the old policies (i.e. taking the best Q values for each) and derive a policy at least as good as the best policy you already have.

There's some additional guarantees here that if the distance between your new task $w_i$ and the closest old task $w_j$ is small, that you'll get a bounded error on the new task using this procedure. 

** Code

*** CleanRL dqn_atari

code is [[https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py][here]]. This is a ~200 line implementation of the original DQN experiment, using convolutional nns etc. 

[[../images/from_clipboard/20241029_114628.png]]

learning_starts is an interesting detail here, I suppose to flood the replay buffer before starting to train. This target_max * (1 - data.dones.flatten()) part is a one-liner which sets the terminal transition states to just be the reward, per algorithm 1 in the paper.

Likewise implementing this using a frozen "target network" and optimizing a separate "q network" is interesting. They seem to update this every 1000 steps or so, but it's a nice trick.

[[../images/from_clipboard/20241029_114822.png]]

Specifically, they copy this over to the target network using this soft update mechanism where you essentially have the target network chase the current values of the Q network. I think this comes from the [[https://arxiv.org/pdf/1509.02971][DDPG paper]] in 2016, which from a quick skim-through looks like it is intended to make divergence less likely. I'll give that a closer look when I get to that paper though.

*** CleanRL PPO

Code is [[https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py][here]]. This code just is for cartpole.

**** Core Idea

[[../images/from_clipboard/20241102_235702.png]]

Critic estimates value (regression problem), Actor selects action (here discrete, but optionally so)

[[../images/from_clipboard/20241103_002142.png]]

Get T timesteps from each actor (i.e. one actor)

[[../images/from_clipboard/20241103_003738.png]]

Get advantages

[[../images/from_clipboard/20241103_005013.png]]

[[../images/from_clipboard/20241103_005146.png]]

over epochs and minibatches, optimize clipped surrogate objective + simple regression objective for value network.

**** Implementation Tricks

The layer init using orthogonal initialization + using different std for actor and critic is apparently common practice in PPO

#+BEGIN_SRC python
def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
#+END_SRC

Clipping values, clipping gradients by global norm, and normalizing advantage inside each minibatch are tricks to this not mentioned in the original PPO paper. There's also early stopping if KL divergence gets too high, and learning rate annealing, which seem helpful as well.
